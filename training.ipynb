{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from dino_game_env import DinoGameEnv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ENVS = 5\n",
    "\n",
    "LOGS_DIR = \"./logs/\"\n",
    "MODELS_DIR = \"./models/\"\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def make_env(vec_env_id):\n",
    "  env = DinoGameEnv(vec_env_id, headless=False)\n",
    "  env = MaxAndSkipEnv(env, skip=2) # skip some frames to limit the number of actions per second (makes training faster)\n",
    "  return env\n",
    "\n",
    "def make_vec_env(num_envs):\n",
    "  def _init_env():\n",
    "    return make_env(vec_env_id=np.random.randint(0, 10000))\n",
    "  return SubprocVecEnv([_init_env for _ in range(num_envs)])\n",
    "\n",
    "training_env = VecMonitor(make_vec_env(N_ENVS), filename=LOGS_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create callback to save the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.callbacks import BaseCallback\n",
    "# from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "# class SaveBestModelsCallback(BaseCallback):\n",
    "#   def __init__(self, check_freq: int, log_dir: str, models_dir: str, verbose: int = 1):\n",
    "#     super(SaveBestModelsCallback, self).__init__(verbose)\n",
    "#     self.check_freq = check_freq\n",
    "#     self.log_dir = log_dir\n",
    "#     self.models_dir = models_dir\n",
    "#     self.best_mean_reward = -np.inf\n",
    "\n",
    "#   def _on_step(self) -> bool:\n",
    "#     if self.n_calls % self.check_freq == 0:\n",
    "#       # Retrieve training reward\n",
    "#       x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "#       if len(x) > 0:\n",
    "#         # Mean training reward over the last 500 episodes\n",
    "#         mean_reward = np.mean(y[-500:])\n",
    "#         print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "#         print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "#         # New best model, you could save the agent here\n",
    "#         if mean_reward > self.best_mean_reward:\n",
    "#           self.best_mean_reward = mean_reward\n",
    "#           # Example for saving best model\n",
    "#           best_model = f\"best_model_{self.num_timesteps}\"\n",
    "#           print(f\"Saving new best model to {self.models_dir}{best_model}\")\n",
    "#           self.model.save(self.models_dir + best_model)\n",
    "#     return True\n",
    "  \n",
    "# callback = SaveBestModelsCallback(check_freq=5000, log_dir=LOGS_DIR, models_dir=MODELS_DIR)\n",
    "\n",
    "callback = CheckpointCallback(\n",
    "  save_freq=20000/N_ENVS,\n",
    "  save_path=MODELS_DIR,\n",
    "  name_prefix=\"best_model\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the PPO agent and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo_agent = PPO(\"MlpPolicy\", env=training_env, learning_rate=0.00003,verbose=1, n_steps=512, tensorboard_log=LOGS_DIR, device=\"cuda\")\n",
    "ppo_agent = PPO.load(path=\"./models/best_model_640000_steps.zip\", env=training_env)\n",
    "\n",
    "ppo_agent.learn(total_timesteps=2e6, callback=callback, tb_log_name=\"PPO_0-00003\", reset_num_timesteps=False)\n",
    "\n",
    "training_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "feb803e7f3d4c09a4ee4368b5c2fdf3b48af001cbe513dcda7f24da28d5b769e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
